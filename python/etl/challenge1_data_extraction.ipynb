{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1: ETL Pipeline - Data Extraction from PostgreSQL\n",
    "\n",
    "## Task Description\n",
    "In this challenge, we need to:\n",
    "1. Connect to PostgreSQL database\n",
    "2. Extract data from multiple tables\n",
    "3. Optimize data loading strategies\n",
    "4. Handle schema discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Pipeline - Data Extraction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostgreSQL Connection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Set up connection parameters\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/datamart\"\n",
    "connection_properties = {\n",
    "    \"user\": \"spark\",\n",
    "    \"password\": \"spark\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# TODO: Validate connection\n",
    "# Hint: Try a simple query to verify connection works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Extract data from the customers table\n",
    "customers_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"raw.customers\") \\\n",
    "    .option(\"user\", connection_properties[\"user\"]) \\\n",
    "    .option(\"password\", connection_properties[\"password\"]) \\\n",
    "    .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "# Display schema and sample data\n",
    "customers_df.printSchema()\n",
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Use partitioning to load data in parallel\n",
    "# Hint: Use numPartitions, partitionColumn, lowerBound, upperBound options\n",
    "\n",
    "orders_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"raw.orders\") \\\n",
    "    .option(\"user\", connection_properties[\"user\"]) \\\n",
    "    .option(\"password\", connection_properties[\"password\"]) \\\n",
    "    .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"partitionColumn\", \"order_id\") \\\n",
    "    .option(\"lowerBound\", 1) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "# Check number of partitions\n",
    "print(f\"Number of partitions: {orders_df.rdd.getNumPartitions()}\")\n",
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Query Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Extract data using a custom SQL query\n",
    "# Hint: Use the query option instead of dbtable\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT o.order_id, o.customer_id, o.order_date, \n",
    "       SUM(oi.quantity * oi.price) as total_amount,\n",
    "       COUNT(oi.order_item_id) as item_count\n",
    "FROM raw.orders o\n",
    "JOIN raw.order_items oi ON o.order_id = oi.order_id\n",
    "GROUP BY o.order_id, o.customer_id, o.order_date\n",
    "ORDER BY o.order_date DESC\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "order_summary_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"query\", query) \\\n",
    "    .option(\"user\", connection_properties[\"user\"]) \\\n",
    "    .option(\"password\", connection_properties[\"password\"]) \\\n",
    "    .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "order_summary_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Write a function to discover all tables in the database\n",
    "# Hint: Query the information_schema.tables view\n",
    "\n",
    "def get_schema_tables(schema_name):\n",
    "    \"\"\"Get all tables for a given schema\"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = '{schema_name}'\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .option(\"user\", connection_properties[\"user\"]) \\\n",
    "        .option(\"password\", connection_properties[\"password\"]) \\\n",
    "        .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "        .load()\n",
    "\n",
    "# Get all tables in the 'raw' schema\n",
    "raw_tables = get_schema_tables(\"raw\")\n",
    "raw_tables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Implement a function for incremental loading based on a timestamp column\n",
    "# Hint: Store the last processed timestamp and only load newer data\n",
    "\n",
    "def load_incremental_data(table_name, timestamp_col, last_processed_time):\n",
    "    \"\"\"Load only data newer than last_processed_time\"\"\"\n",
    "    \n",
    "    # Format the timestamp for SQL\n",
    "    formatted_time = last_processed_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Create query with timestamp filter\n",
    "    query = f\"SELECT * FROM {table_name} WHERE {timestamp_col} > '{formatted_time}'\"\n",
    "    \n",
    "    # Load data\n",
    "    return spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .option(\"user\", connection_properties[\"user\"]) \\\n",
    "        .option(\"password\", connection_properties[\"password\"]) \\\n",
    "        .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "        .load()\n",
    "\n",
    "# Example usage (comment out if tables don't exist yet)\n",
    "# from datetime import datetime\n",
    "# last_processed = datetime(2023, 1, 1)\n",
    "# new_orders = load_incremental_data(\"raw.orders\", \"order_date\", last_processed)\n",
    "# new_orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Implement extraction within a database transaction\n",
    "# Note: This requires using the PySpark JDBC internals or the psycopg2 library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
