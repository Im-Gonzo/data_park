{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2: ETL Pipeline - Data Transformation\n",
    "\n",
    "## Task Description\n",
    "In this challenge, we need to:\n",
    "1. Apply transformations to extracted data\n",
    "2. Handle data quality issues\n",
    "3. Optimize transformation operations\n",
    "4. Prepare data for loading phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Pipeline - Data Transformation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from Challenge 1\n",
    "\n",
    "Let's assume we have the DataFrames from Challenge 1. If not, we'll recreate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# PostgreSQL connection parameters\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/datamart\"\n",
    "connection_properties = {\n",
    "    \"user\": \"spark\",\n",
    "    \"password\": \"spark\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Load tables\n",
    "customers_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"raw.customers\") \\\n",
    "    .option(\"user\", connection_properties[\"user\"]) \\\n",
    "    .option(\"password\", connection_properties[\"password\"]) \\\n",
    "    .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "orders_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"raw.orders\") \\\n",
    "    .option(\"user\", connection_properties[\"user\"]) \\\n",
    "    .option(\"password\", connection_properties[\"password\"]) \\\n",
    "    .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "order_items_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"raw.order_items\") \\\n",
    "    .option(\"user\", connection_properties[\"user\"]) \\\n",
    "    .option(\"password\", connection_properties[\"password\"]) \\\n",
    "    .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "products_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"raw.products\") \\\n",
    "    .option(\"user\", connection_properties[\"user\"]) \\\n",
    "    .option(\"password\", connection_properties[\"password\"]) \\\n",
    "    .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Handle null values in customer data\n",
    "clean_customers_df = customers_df \\\n",
    "    .filter(col(\"email\").isNotNull()) \\\n",
    "    .withColumn(\"first_name\", when(col(\"first_name\").isNull(), \"Unknown\").otherwise(col(\"first_name\"))) \\\n",
    "    .withColumn(\"last_name\", when(col(\"last_name\").isNull(), \"Unknown\").otherwise(col(\"last_name\"))) \\\n",
    "    .withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "# Check results\n",
    "clean_customers_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Standardize data formats\n",
    "standardized_orders_df = orders_df \\\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_date\"))) \\\n",
    "    .withColumn(\"order_year\", year(col(\"order_date\"))) \\\n",
    "    .withColumn(\"order_month\", month(col(\"order_date\"))) \\\n",
    "    .withColumn(\"order_day\", dayofmonth(col(\"order_date\"))) \\\n",
    "    .withColumn(\"status\", upper(col(\"status\"))) \\\n",
    "    .withColumn(\"is_completed\", when(col(\"status\") == \"COMPLETED\", True).otherwise(False))\n",
    "\n",
    "# Check results\n",
    "standardized_orders_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Join datasets to create enriched view\n",
    "# Calculate order totals and join with customer information\n",
    "\n",
    "# First calculate order totals\n",
    "order_totals_df = order_items_df \\\n",
    "    .groupBy(\"order_id\") \\\n",
    "    .agg(\n",
    "        sum(col(\"quantity\") * col(\"price\")).alias(\"order_total\"),\n",
    "        count(\"*\").alias(\"num_items\")\n",
    "    )\n",
    "\n",
    "# Join orders with totals\n",
    "orders_with_totals = standardized_orders_df \\\n",
    "    .join(order_totals_df, \"order_id\")\n",
    "\n",
    "# Join with customer data\n",
    "enriched_orders_df = orders_with_totals \\\n",
    "    .join(clean_customers_df, \"customer_id\")\n",
    "\n",
    "# Select final columns\n",
    "final_enriched_df = enriched_orders_df.select(\n",
    "    \"order_id\",\n",
    "    \"customer_id\",\n",
    "    \"full_name\",\n",
    "    \"email\",\n",
    "    \"order_date\",\n",
    "    \"order_year\",\n",
    "    \"order_month\",\n",
    "    \"status\",\n",
    "    \"order_total\",\n",
    "    \"num_items\"\n",
    ")\n",
    "\n",
    "# Check results\n",
    "final_enriched_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Transformation: Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Implement customer segmentation logic\n",
    "# Calculate metrics like total spend, frequency, recency\n",
    "\n",
    "# Aggregate metrics by customer\n",
    "customer_metrics = final_enriched_df \\\n",
    "    .groupBy(\"customer_id\", \"full_name\", \"email\") \\\n",
    "    .agg(\n",
    "        sum(\"order_total\").alias(\"total_spend\"),\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        max(\"order_date\").alias(\"last_order_date\")\n",
    "    )\n",
    "\n",
    "# Calculate recency in days\n",
    "customer_metrics = customer_metrics \\\n",
    "    .withColumn(\"recency_days\", datediff(current_date(), col(\"last_order_date\")))\n",
    "\n",
    "# Assign segments\n",
    "customer_segments = customer_metrics \\\n",
    "    .withColumn(\"spend_segment\", \n",
    "               when(col(\"total_spend\") > 1000, \"High\")\n",
    "               .when(col(\"total_spend\") > 500, \"Medium\")\n",
    "               .otherwise(\"Low\")) \\\n",
    "    .withColumn(\"frequency_segment\", \n",
    "               when(col(\"order_count\") > 10, \"High\")\n",
    "               .when(col(\"order_count\") > 5, \"Medium\")\n",
    "               .otherwise(\"Low\")) \\\n",
    "    .withColumn(\"recency_segment\", \n",
    "               when(col(\"recency_days\") < 30, \"Active\")\n",
    "               .when(col(\"recency_days\") < 90, \"Recent\")\n",
    "               .otherwise(\"Inactive\"))\n",
    "\n",
    "# Create overall segment\n",
    "customer_segments = customer_segments \\\n",
    "    .withColumn(\"customer_segment\", \n",
    "               when((col(\"spend_segment\") == \"High\") & (col(\"recency_segment\") == \"Active\"), \"VIP\")\n",
    "               .when((col(\"spend_segment\") == \"High\") | (col(\"frequency_segment\") == \"High\"), \"Premium\")\n",
    "               .when((col(\"recency_segment\") == \"Inactive\"), \"At Risk\")\n",
    "               .otherwise(\"Regular\"))\n",
    "\n",
    "# Check results\n",
    "customer_segments.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Optimize transformations using caching, partitioning, etc.\n",
    "\n",
    "# Cache frequently used DataFrames\n",
    "clean_customers_df.cache()\n",
    "standardized_orders_df.cache()\n",
    "\n",
    "# Repartition for better parallelism\n",
    "num_partitions = 8\n",
    "repartitioned_orders = standardized_orders_df.repartition(num_partitions, \"customer_id\")\n",
    "repartitioned_customers = clean_customers_df.repartition(num_partitions, \"customer_id\")\n",
    "\n",
    "# Perform join with optimized partitioning\n",
    "optimized_join = repartitioned_orders \\\n",
    "    .join(repartitioned_customers, \"customer_id\")\n",
    "\n",
    "# Check partition count\n",
    "print(f\"Original orders partitions: {standardized_orders_df.rdd.getNumPartitions()}\")\n",
    "print(f\"Repartitioned orders: {repartitioned_orders.rdd.getNumPartitions()}\")\n",
    "print(f\"Optimized join partitions: {optimized_join.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Implement schema evolution handling\n",
    "\n",
    "def add_missing_columns(df, required_columns):\n",
    "    \"\"\"Add any missing columns to the DataFrame with null values\"\"\"\n",
    "    current_columns = df.columns\n",
    "    for column in required_columns:\n",
    "        if column not in current_columns:\n",
    "            df = df.withColumn(column, lit(None))\n",
    "    return df\n",
    "\n",
    "def select_required_columns(df, required_columns):\n",
    "    \"\"\"Select only the required columns in the specified order\"\"\"\n",
    "    # First ensure all required columns exist\n",
    "    df_with_all_columns = add_missing_columns(df, required_columns)\n",
    "    # Then select only required columns in correct order\n",
    "    return df_with_all_columns.select(*required_columns)\n",
    "\n",
    "# Example usage\n",
    "required_order_columns = [\"order_id\", \"customer_id\", \"order_date\", \"status\", \"total\", \"tax\", \"shipping\"]\n",
    "\n",
    "# standardized_orders_with_schema_handling = select_required_columns(standardized_orders_df, required_order_columns)\n",
    "# standardized_orders_with_schema_handling.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
