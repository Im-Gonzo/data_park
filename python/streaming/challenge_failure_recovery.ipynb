{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Failure Recovery and Checkpoint Management\n",
    "\n",
    "## Task Description\n",
    "In this challenge, we need to:\n",
    "1. Set up checkpointing for Spark Structured Streaming\n",
    "2. Implement recovery mechanisms for partial failures\n",
    "3. Properly manage Kafka offsets\n",
    "4. Ensure exactly-once delivery semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Fault Tolerant Streaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 8) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Checkpointing\n",
    "\n",
    "Checkpointing is essential for fault tolerance in streaming applications. It allows Spark to recover after a failure by storing state information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define checkpoint directory\n",
    "checkpoint_dir = \"/tmp/spark-checkpoints/vpc-security\"\n",
    "\n",
    "# TODO: Create checkpoint directory if it doesn't exist\n",
    "# Note: In a production environment, this would typically be on HDFS, S3, or another distributed filesystem\n",
    "# For this notebook, we'll use a local directory\n",
    "\n",
    "import os\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    print(f\"Created checkpoint directory: {checkpoint_dir}\")\n",
    "else:\n",
    "    print(f\"Checkpoint directory already exists: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Schema for Streaming Data\n",
    "\n",
    "We'll use the same schema as in previous challenges for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define schema for VPN connection events\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"ip_address\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"duration_seconds\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Fault-Tolerant Streaming Query\n",
    "\n",
    "Let's set up a streaming query with proper checkpointing and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Create a function to build a streaming query with fault tolerance\n",
    "def create_fault_tolerant_stream(checkpoint_location):\n",
    "    \"\"\"Create a streaming query with checkpointing for fault tolerance\"\"\"\n",
    "    \n",
    "    # Read from Kafka with explicit offsets for recovery\n",
    "    stream_df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"subscribe\", \"vpn_connection_events\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Parse JSON data\n",
    "    parsed_df = stream_df \\\n",
    "        .selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "        .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\")\n",
    "    \n",
    "    # Add event timestamp for window operations\n",
    "    timestamped_df = parsed_df \\\n",
    "        .withColumn(\"event_time\", to_timestamp(col(\"timestamp\"))) \\\n",
    "        .withWatermark(\"event_time\", \"10 minutes\")\n",
    "    \n",
    "    # Perform aggregation\n",
    "    aggregated_df = timestamped_df \\\n",
    "        .groupBy(\n",
    "            window(col(\"event_time\"), \"5 minutes\"),\n",
    "            col(\"country\")\n",
    "        ) \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"connection_count\"),\n",
    "            sum(when(col(\"status\") == \"success\", 1).otherwise(0)).alias(\"successful_connections\"),\n",
    "            sum(when(col(\"status\") == \"failed\", 1).otherwise(0)).alias(\"failed_connections\")\n",
    "        )\n",
    "    \n",
    "    # Start the streaming query with checkpoint location\n",
    "    query = aggregated_df \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "        .start()\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exactly-Once Semantics with Idempotent Sink\n",
    "\n",
    "To achieve exactly-once semantics, we need both checkpointing and an idempotent sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Implement exactly-once semantics with foreachBatch\n",
    "def create_exactly_once_stream(checkpoint_location):\n",
    "    \"\"\"Create a streaming query with exactly-once semantics\"\"\"\n",
    "    \n",
    "    # Read from Kafka\n",
    "    stream_df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"subscribe\", \"vpn_connection_events\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Parse JSON data\n",
    "    parsed_df = stream_df \\\n",
    "        .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING) as json\", \"topic\", \"partition\", \"offset\") \\\n",
    "        .select(\n",
    "            col(\"key\"),\n",
    "            from_json(col(\"json\"), schema).alias(\"data\"),\n",
    "            col(\"topic\"),\n",
    "            col(\"partition\"),\n",
    "            col(\"offset\")\n",
    "        ) \\\n",
    "        .select(\n",
    "            col(\"topic\"),\n",
    "            col(\"partition\"),\n",
    "            col(\"offset\"),\n",
    "            col(\"data.*\")\n",
    "        )\n",
    "    \n",
    "    # Add timestamp\n",
    "    processed_df = parsed_df \\\n",
    "        .withColumn(\"event_time\", to_timestamp(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"processing_time\", current_timestamp())\n",
    "    \n",
    "    # Define a foreachBatch function with idempotent write logic\n",
    "    def process_batch(batch_df, batch_id):\n",
    "        if batch_df.isEmpty():\n",
    "            print(f\"Batch {batch_id} is empty, skipping\")\n",
    "            return\n",
    "        \n",
    "        # Generate batch metrics\n",
    "        metrics_df = batch_df \\\n",
    "            .groupBy(\"country\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"connection_count\"),\n",
    "                max(\"offset\").alias(\"max_offset\"),\n",
    "                min(\"offset\").alias(\"min_offset\")\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            # In a real application, this would write to a transactional sink\n",
    "            # For PostgreSQL, we would use a transaction\n",
    "            print(f\"\\nProcessing batch {batch_id} with {batch_df.count()} records\")\n",
    "            \n",
    "            # Example: Idempotent write to PostgreSQL using a unique constraint\n",
    "            # This is a simulation:\n",
    "            print(\"Metrics by country:\")\n",
    "            metrics_df.show()\n",
    "            \n",
    "            # Simulate writing to PostgreSQL with transaction\n",
    "            print(f\"Batch {batch_id} processed successfully\")\n",
    "            print(f\"Committed offsets up to {metrics_df.agg({'max_offset': 'max'}).collect()[0][0]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # In case of failure, the entire batch will be retried due to checkpointing\n",
    "            print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    # Start streaming query with exactly-once semantics\n",
    "    query = processed_df \\\n",
    "        .writeStream \\\n",
    "        .foreachBatch(process_batch) \\\n",
    "        .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "        .start()\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovery Simulation\n",
    "\n",
    "Let's simulate a failure and recovery scenario to test our fault tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Simulate a failure and recovery\n",
    "def test_failure_recovery():\n",
    "    checkpoint_subdir = f\"{checkpoint_dir}/recovery-test-{int(time.time())}\"\n",
    "    print(f\"Starting streaming query with checkpoint at {checkpoint_subdir}\")\n",
    "    \n",
    "    # Start a query\n",
    "    query = create_exactly_once_stream(checkpoint_subdir)\n",
    "    \n",
    "    # Let it run for a few seconds\n",
    "    print(\"Running query for 10 seconds...\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Simulate a failure by stopping the query\n",
    "    print(\"\\nSimulating failure by stopping the query...\")\n",
    "    query.stop()\n",
    "    \n",
    "    # Wait a moment\n",
    "    print(\"Waiting 5 seconds before recovery...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Restart the query with the same checkpoint dir\n",
    "    print(\"\\nRestarting query with the same checkpoint location...\")\n",
    "    recovered_query = create_exactly_once_stream(checkpoint_subdir)\n",
    "    \n",
    "    # Let it run again to show recovery\n",
    "    print(\"Running recovered query for 20 seconds...\")\n",
    "    time.sleep(20)\n",
    "    \n",
    "    # Stop the recovered query\n",
    "    print(\"\\nStopping recovered query...\")\n",
    "    recovered_query.stop()\n",
    "    \n",
    "    print(\"\\nFailure recovery test complete\")\n",
    "    print(f\"Check {checkpoint_subdir} to see the checkpoint files created\")\n",
    "\n",
    "# Run the test\n",
    "# Note: Comment this out during development and only run it when the Kafka producer is running\n",
    "# test_failure_recovery()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Kafka Offsets for Recovery\n",
    "\n",
    "Understanding how Kafka offsets are managed is crucial for proper recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Explore Kafka offset management\n",
    "def explain_kafka_offset_management():\n",
    "    \"\"\"Explain how Kafka offsets are managed for fault tolerance\"\"\"\n",
    "    \n",
    "    # Create a sample stream for demonstration\n",
    "    stream_df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"subscribe\", \"vpn_connection_events\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .load() \\\n",
    "        .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"topic\", \"partition\", \"offset\")\n",
    "    \n",
    "    # Explain the query plan to show how offsets are tracked\n",
    "    print(\"Query plan showing how Kafka offsets are tracked:\")\n",
    "    stream_df.explain(True)\n",
    "    \n",
    "    print(\"\\nKey Kafka offset options for fault tolerance:\")\n",
    "    print(\"1. startingOffsets: Where to start reading when no checkpoint exists\")\n",
    "    print(\"   - 'earliest': Start from the beginning of the topic\")\n",
    "    print(\"   - 'latest': Start from the end of the topic\")\n",
    "    print(\"   - '{\\\"topicA\\\":{\\\"0\\\":23,\\\"1\\\":-1},\\\"topicB\\\":{\\\"0\\\":-1}}': Specific offsets per partition\")\n",
    "    \n",
    "    print(\"\\n2. failOnDataLoss: How to handle data that is no longer available\")\n",
    "    print(\"   - 'true': Fail the query if data is lost (e.g., due to retention policies)\")\n",
    "    print(\"   - 'false': Continue the query even if some data is lost\")\n",
    "    \n",
    "    print(\"\\n3. checkpointLocation: Where to store offset information\")\n",
    "    print(\"   - Stores the latest processed offset per partition\")\n",
    "    print(\"   - Used to resume from where processing left off after a failure\")\n",
    "    \n",
    "    print(\"\\nWhen a failure occurs and the query restarts:\")\n",
    "    print(\"1. Spark checks the checkpoint location for the latest committed offsets\")\n",
    "    print(\"2. It resumes processing from those offsets, ensuring no data loss\")\n",
    "    print(\"3. With exactly-once semantics, it ensures no duplicate processing\")\n",
    "\n",
    "# Explain Kafka offset management\n",
    "# explain_kafka_offset_management()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Checkpoint Files\n",
    "\n",
    "Let's look at what's stored in the checkpoint directory to better understand the recovery mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Explore checkpoint directory structure\n",
    "def explore_checkpoint_directory(dir_path):\n",
    "    \"\"\"Explore the structure of a checkpoint directory\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        print(f\"Directory {dir_path} does not exist\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Contents of {dir_path}:\")\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        level = root.replace(dir_path, '').count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        sub_indent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f\"{sub_indent}{f}\")\n",
    "    \n",
    "    # Explain key checkpoint files\n",
    "    print(\"\\nKey checkpoint files and directories:\")\n",
    "    print(\"- offsets/: Contains the offsets that have been processed\")\n",
    "    print(\"- commits/: Tracks successful batch completions\")\n",
    "    print(\"- metadata/: Stores query metadata\")\n",
    "    print(\"- sources/: Contains information about the data sources\")\n",
    "    print(\"- state/: Maintains stateful operator state (for aggregations, windowing, etc.)\")\n",
    "\n",
    "# Explore a sample checkpoint directory\n",
    "# Note: This requires running a query first to create checkpoint files\n",
    "# explore_checkpoint_directory(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Robust Streaming Pipeline\n",
    "\n",
    "Let's combine all our fault tolerance techniques into a production-ready streaming pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Create a robust, fault-tolerant streaming pipeline\n",
    "def create_robust_pipeline():\n",
    "    \"\"\"Create a robust, fault-tolerant streaming pipeline for production use\"\"\"\n",
    "    \n",
    "    # Define checkpoint location\n",
    "    robust_checkpoint_dir = f\"{checkpoint_dir}/production-pipeline\"\n",
    "    \n",
    "    # Read from Kafka with fault tolerance options\n",
    "    raw_stream = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"subscribe\", \"vpn_connection_events\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .option(\"kafkaConsumer.pollTimeoutMs\", \"5000\") \\\n",
    "        .option(\"fetchOffset.numRetries\", \"5\") \\\n",
    "        .option(\"fetchOffset.retryIntervalMs\", \"1000\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Parse and process data\n",
    "    parsed_stream = raw_stream \\\n",
    "        .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING) as json\", \"topic\", \"partition\", \"offset\") \\\n",
    "        .select(\n",
    "            col(\"key\"),\n",
    "            from_json(col(\"json\"), schema).alias(\"data\"),\n",
    "            col(\"topic\"),\n",
    "            col(\"partition\"),\n",
    "            col(\"offset\")\n",
    "        ) \\\n",
    "        .select(\n",
    "            col(\"topic\"),\n",
    "            col(\"partition\"),\n",
    "            col(\"offset\"),\n",
    "            col(\"data.*\")\n",
    "        )\n",
    "    \n",
    "    # Add timestamps and watermarks for stateful processing\n",
    "    timestamped_stream = parsed_stream \\\n",
    "        .withColumn(\"event_time\", to_timestamp(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"processing_time\", current_timestamp()) \\\n",
    "        .withWatermark(\"event_time\", \"30 minutes\")  # Allow for late data\n",
    "    \n",
    "    # Implement error handling and retry logic\n",
    "    def process_batch_with_retries(batch_df, batch_id):\n",
    "        if batch_df.isEmpty():\n",
    "            return\n",
    "        \n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                # Start a transaction (in a real system)\n",
    "                print(f\"Processing batch {batch_id} (attempt {retry_count + 1})\")\n",
    "                \n",
    "                # Process data\n",
    "                country_metrics = batch_df \\\n",
    "                    .groupBy(\"country\") \\\n",
    "                    .agg(\n",
    "                        count(\"*\").alias(\"connection_count\"),\n",
    "                        sum(when(col(\"status\") == \"success\", 1).otherwise(0)).alias(\"successful\"),\n",
    "                        sum(when(col(\"status\") == \"failed\", 1).otherwise(0)).alias(\"failed\")\n",
    "                    )\n",
    "                \n",
    "                # In a real system, this would write to a database with a transaction\n",
    "                print(\"Metrics by country:\")\n",
    "                country_metrics.show(5, truncate=False)\n",
    "                \n",
    "                # Commit transaction (in a real system)\n",
    "                print(f\"Successfully processed batch {batch_id}\")\n",
    "                return\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                if retry_count >= max_retries:\n",
    "                    print(f\"Failed to process batch {batch_id} after {max_retries} attempts. Error: {str(e)}\")\n",
    "                    # In a production system, would log to error tracking system\n",
    "                    # and possibly write to a dead-letter queue\n",
    "                    raise e\n",
    "                else:\n",
    "                    print(f\"Attempt {retry_count} failed. Retrying... Error: {str(e)}\")\n",
    "                    time.sleep(1)  # Backoff before retry\n",
    "    \n",
    "    # Start the query with fault tolerance features\n",
    "    query = timestamped_stream \\\n",
    "        .writeStream \\\n",
    "        .foreachBatch(process_batch_with_retries) \\\n",
    "        .option(\"checkpointLocation\", robust_checkpoint_dir) \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    return query\n",
    "\n",
    "# Start the robust pipeline\n",
    "# Note: Only run this when ready to test with real data\n",
    "# robust_query = create_robust_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Fault-Tolerant Streaming\n",
    "\n",
    "Here are some key best practices for building fault-tolerant streaming applications:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Always use checkpointing**: This is essential for recovery after failures\n",
    "\n",
    "2. **Configure failure handling**:\n",
    "   - Set `failOnDataLoss` to `false` for production systems\n",
    "   - Implement retry logic with backoff for transient errors\n",
    "\n",
    "3. **Use a reliable storage for checkpoints**:\n",
    "   - HDFS, S3, or other distributed storage for production\n",
    "   - Ensure the checkpoint location is accessible by all nodes\n",
    "\n",
    "4. **Implement idempotent sinks**:\n",
    "   - Use `foreachBatch` with transaction support\n",
    "   - Ensure writes are idempotent (e.g., using unique constraints)\n",
    "\n",
    "5. **Monitor and alert**:\n",
    "   - Track streaming metrics\n",
    "   - Set up alerting for failures\n",
    "   - Keep logs for debugging\n",
    "\n",
    "6. **Test failure scenarios**:\n",
    "   - Simulate node failures\n",
    "   - Test recovery from checkpoint\n",
    "   - Validate exactly-once semantics\n",
    "\n",
    "7. **Scale appropriately**:\n",
    "   - Size your cluster for peak loads plus buffer\n",
    "   - Use dynamic allocation if available\n",
    "   - Tune executor memory and CPU\n",
    "\n",
    "8. **Version your data schemas**:\n",
    "   - Have a plan for schema evolution\n",
    "   - Test backward compatibility\n",
    "\n",
    "9. **Implement dead-letter queues**:\n",
    "   - Store records that cannot be processed\n",
    "   - Investigate and reprocess later\n",
    "\n",
    "10. **Validate end-to-end**:\n",
    "    - Ensure data consistency\n",
    "    - Check for duplicate or missing data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
