{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1: Detecting Data Skew in Connection Logs\n",
    "\n",
    "## Task Description\n",
    "In this challenge, we need to:\n",
    "1. Analyze connection log data to identify skew\n",
    "2. Detect imbalanced distributions across countries/regions\n",
    "3. Visualize and quantify the skew\n",
    "4. Understand the impact on processing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Skew Detection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Connection Log Data\n",
    "\n",
    "First, let's connect to our database to load VPN connection logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# PostgreSQL connection parameters\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/datamart\"\n",
    "connection_properties = {\n",
    "    \"user\": \"spark\",\n",
    "    \"password\": \"spark\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# TODO: Load connection logs from PostgreSQL\n",
    "# If your table doesn't exist yet, create sample data instead\n",
    "\n",
    "# Option 1: Load from database if table exists\n",
    "try:\n",
    "    connection_logs = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"raw.connection_logs\") \\\n",
    "        .option(\"user\", connection_properties[\"user\"]) \\\n",
    "        .option(\"password\", connection_properties[\"password\"]) \\\n",
    "        .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "        .load()\n",
    "    \n",
    "    print(f\"Loaded {connection_logs.count()} records from database\")\n",
    "    \n",
    "except:\n",
    "    print(\"Table not found, creating sample data instead\")\n",
    "    \n",
    "    # Option 2: Create sample data with skew\n",
    "    # We'll create data with heavy skew toward certain countries\n",
    "    countries = [\"US\", \"UK\", \"DE\", \"FR\", \"CN\", \"IN\", \"BR\", \"JP\", \"CA\", \"AU\"]\n",
    "    \n",
    "    # Create sample data with skew\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # Generate connection records with skew\n",
    "    num_records = 100000\n",
    "    \n",
    "    skewed_country = \"US\"  # This country will have most of the records\n",
    "    skew_percentage = 0.7  # 70% of records will be for this country\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        # Determine country with skew\n",
    "        if random.random() < skew_percentage:\n",
    "            country = skewed_country\n",
    "        else:\n",
    "            country = random.choice([c for c in countries if c != skewed_country])\n",
    "        \n",
    "        # Create record\n",
    "        timestamp = datetime.now() - timedelta(days=random.randint(0, 30), \n",
    "                                              hours=random.randint(0, 23),\n",
    "                                              minutes=random.randint(0, 59))\n",
    "        \n",
    "        data.append((\n",
    "            f\"user_{random.randint(1, 1000)}\",  # user_id\n",
    "            timestamp.isoformat(),              # timestamp\n",
    "            country,                            # country\n",
    "            f\"10.{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}\",  # ip_address\n",
    "            random.choice([\"success\", \"failed\"]), # status\n",
    "            random.randint(1, 100)              # duration_seconds\n",
    "        ))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    columns = [\"user_id\", \"timestamp\", \"country\", \"ip_address\", \"status\", \"duration_seconds\"]\n",
    "    connection_logs = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    print(f\"Created {connection_logs.count()} sample records with skewed distribution\")\n",
    "\n",
    "# Cache for better performance\n",
    "connection_logs.cache()\n",
    "connection_logs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Skew by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Analyze data distribution by country\n",
    "country_distribution = connection_logs \\\n",
    "    .groupBy(\"country\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\"))\n",
    "\n",
    "# Calculate skew metrics\n",
    "total_records = connection_logs.count()\n",
    "country_distribution_with_pct = country_distribution \\\n",
    "    .withColumn(\"percentage\", (col(\"count\") / lit(total_records)) * 100) \\\n",
    "    .withColumn(\"percentage\", round(col(\"percentage\"), 2))\n",
    "\n",
    "country_distribution_with_pct.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Create a bar chart showing record count by country\n",
    "# Collect data for plotting\n",
    "plot_data = country_distribution.collect()\n",
    "countries = [row[\"country\"] for row in plot_data]\n",
    "counts = [row[\"count\"] for row in plot_data]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(countries, counts)\n",
    "plt.title('Connection Log Distribution by Country')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Records')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Partition Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Analyze how the skew affects partitioning\n",
    "# Create a larger number of partitions to observe skew\n",
    "num_partitions = 8\n",
    "\n",
    "# Repartition based on skewed key (country)\n",
    "skewed_partitions = connection_logs.repartition(num_partitions, \"country\")\n",
    "\n",
    "# Add partition ID for analysis\n",
    "partition_counts = skewed_partitions \\\n",
    "    .withColumn(\"partition_id\", spark_partition_id()) \\\n",
    "    .groupBy(\"partition_id\", \"country\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"partition_id\", desc(\"count\"))\n",
    "\n",
    "partition_counts.show(num_partitions * 3)  # Show multiple rows per partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Performance Impact of Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Measure execution time for an operation on skewed data\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a second dataset to join with\n",
    "country_info = spark.createDataFrame([\n",
    "    (\"US\", \"United States\", \"North America\", \"English\"),\n",
    "    (\"UK\", \"United Kingdom\", \"Europe\", \"English\"),\n",
    "    (\"DE\", \"Germany\", \"Europe\", \"German\"),\n",
    "    (\"FR\", \"France\", \"Europe\", \"French\"),\n",
    "    (\"CN\", \"China\", \"Asia\", \"Chinese\"),\n",
    "    (\"IN\", \"India\", \"Asia\", \"Hindi/English\"),\n",
    "    (\"BR\", \"Brazil\", \"South America\", \"Portuguese\"),\n",
    "    (\"JP\", \"Japan\", \"Asia\", \"Japanese\"),\n",
    "    (\"CA\", \"Canada\", \"North America\", \"English/French\"),\n",
    "    (\"AU\", \"Australia\", \"Oceania\", \"English\")\n",
    "], [\"country_code\", \"country_name\", \"region\", \"language\"])\n",
    "\n",
    "# Time a standard join (will be affected by skew)\n",
    "start_time = time.time()\n",
    "\n",
    "skewed_join = connection_logs \\\n",
    "    .join(country_info, connection_logs.country == country_info.country_code) \\\n",
    "    .groupBy(\"country_name\", \"region\") \\\n",
    "    .agg(count(\"*\").alias(\"connection_count\"))\n",
    "\n",
    "# Force execution\n",
    "skewed_join.collect()\n",
    "\n",
    "skewed_join_time = time.time() - start_time\n",
    "print(f\"Time to perform join on skewed data: {skewed_join_time:.2f} seconds\")\n",
    "\n",
    "# Display results\n",
    "skewed_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify Skew Using Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Calculate a skew factor using statistical methods\n",
    "# Collect partition statistics\n",
    "partition_stats = skewed_partitions \\\n",
    "    .withColumn(\"partition_id\", spark_partition_id()) \\\n",
    "    .groupBy(\"partition_id\") \\\n",
    "    .count() \\\n",
    "    .agg(\n",
    "        avg(\"count\").alias(\"avg_records_per_partition\"),\n",
    "        stddev(\"count\").alias(\"stddev_records\"),\n",
    "        min(\"count\").alias(\"min_records\"),\n",
    "        max(\"count\").alias(\"max_records\")\n",
    "    )\n",
    "\n",
    "# Calculate coefficient of variation as a measure of skew\n",
    "skew_stats = partition_stats \\\n",
    "    .withColumn(\"coefficient_of_variation\", col(\"stddev_records\") / col(\"avg_records_per_partition\")) \\\n",
    "    .withColumn(\"max_to_min_ratio\", col(\"max_records\") / col(\"min_records\"))\n",
    "\n",
    "skew_stats.show()\n",
    "\n",
    "# Get values for analysis\n",
    "stats = skew_stats.collect()[0]\n",
    "cv = stats[\"coefficient_of_variation\"]\n",
    "ratio = stats[\"max_to_min_ratio\"]\n",
    "\n",
    "print(f\"Coefficient of variation: {cv:.2f}\")\n",
    "print(f\"Max/Min ratio: {ratio:.2f}\")\n",
    "\n",
    "# Interpret results\n",
    "if cv > 1.0:\n",
    "    print(\"High skew detected (CV > 1.0)\")\n",
    "elif cv > 0.5:\n",
    "    print(\"Moderate skew detected (0.5 < CV < 1.0)\")\n",
    "else:\n",
    "    print(\"Low skew detected (CV < 0.5)\")\n",
    "    \n",
    "if ratio > 10:\n",
    "    print(f\"Severe imbalance: busiest partition has {ratio:.1f}x more records than emptiest\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
