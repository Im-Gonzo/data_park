{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3: Optimizing Joins with Broadcast Variables\n",
    "\n",
    "## Task Description\n",
    "In this challenge, we need to:\n",
    "1. Understand when to use broadcast joins\n",
    "2. Implement explicit and automatic broadcast joins\n",
    "3. Measure performance improvements from broadcasting\n",
    "4. Consider implications for larger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Broadcast Join Optimization\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data\n",
    "\n",
    "We'll create two datasets:\n",
    "1. A large dataset of connection logs (many rows)\n",
    "2. A small reference dataset of country information (few rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate a larger dataset for more meaningful performance comparison\n",
    "countries = [\"US\", \"UK\", \"DE\", \"FR\", \"CN\", \"IN\", \"BR\", \"JP\", \"CA\", \"AU\"]\n",
    "    \n",
    "# Create sample data\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "    \n",
    "data = []\n",
    "    \n",
    "# Generate connection records\n",
    "num_records = 500000  # Use more records for better measurement\n",
    "    \n",
    "for i in range(num_records):\n",
    "    # More even distribution for this test\n",
    "    country = random.choice(countries)\n",
    "    \n",
    "    # Create record\n",
    "    timestamp = datetime.now() - timedelta(days=random.randint(0, 30), \n",
    "                                          hours=random.randint(0, 23),\n",
    "                                          minutes=random.randint(0, 59))\n",
    "    \n",
    "    data.append((\n",
    "        f\"user_{random.randint(1, 5000)}\",  # user_id\n",
    "        timestamp.isoformat(),              # timestamp\n",
    "        country,                            # country\n",
    "        f\"10.{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}\",  # ip_address\n",
    "        random.choice([\"success\", \"failed\"]), # status\n",
    "        random.randint(1, 100)              # duration_seconds\n",
    "    ))\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"user_id\", \"timestamp\", \"country\", \"ip_address\", \"status\", \"duration_seconds\"]\n",
    "connection_logs = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Create a small lookup table for countries (this is the table we'll broadcast)\n",
    "country_info = spark.createDataFrame([\n",
    "    (\"US\", \"United States\", \"North America\", \"English\"),\n",
    "    (\"UK\", \"United Kingdom\", \"Europe\", \"English\"),\n",
    "    (\"DE\", \"Germany\", \"Europe\", \"German\"),\n",
    "    (\"FR\", \"France\", \"Europe\", \"French\"),\n",
    "    (\"CN\", \"China\", \"Asia\", \"Chinese\"),\n",
    "    (\"IN\", \"India\", \"Asia\", \"Hindi/English\"),\n",
    "    (\"BR\", \"Brazil\", \"South America\", \"Portuguese\"),\n",
    "    (\"JP\", \"Japan\", \"Asia\", \"Japanese\"),\n",
    "    (\"CA\", \"Canada\", \"North America\", \"English/French\"),\n",
    "    (\"AU\", \"Australia\", \"Oceania\", \"English\")\n",
    "], [\"country_code\", \"country_name\", \"region\", \"language\"])\n",
    "\n",
    "# Cache for better performance\n",
    "connection_logs.cache()\n",
    "connection_logs.count()  # Force caching\n",
    "\n",
    "country_info.cache()\n",
    "country_info.count()  # Force caching\n",
    "\n",
    "print(f\"Created dataset with {connection_logs.count()} connection logs and {country_info.count()} country records\")\n",
    "print(\"Connection logs schema:\")\n",
    "connection_logs.printSchema()\n",
    "print(\"\\nCountry info schema:\")\n",
    "country_info.printSchema()\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample connection logs:\")\n",
    "connection_logs.show(5)\n",
    "print(\"\\nCountry reference data:\")\n",
    "country_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Dataset Sizes\n",
    "\n",
    "It's important to verify that one dataset is much smaller than the other, making it a good candidate for broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Compare dataset sizes to confirm broadcasting is appropriate\n",
    "\n",
    "# Get approximate sizes\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def estimate_size_mb(df, sample_ratio=0.1):\n",
    "    \"\"\"Estimate dataframe size in MB based on sampling\"\"\"\n",
    "    sampled = df.sample(withReplacement=False, fraction=sample_ratio)\n",
    "    serialized_rows = sampled.rdd.map(lambda x: Row(**x.asDict()))\n",
    "    row_size_bytes = serialized_rows.map(lambda x: len(str(x))).mean()\n",
    "    total_size_bytes = row_size_bytes * df.count() / sample_ratio\n",
    "    return total_size_bytes / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Estimate sizes\n",
    "logs_size_mb = estimate_size_mb(connection_logs)\n",
    "country_size_mb = estimate_size_mb(country_info)\n",
    "size_ratio = logs_size_mb / country_size_mb if country_size_mb > 0 else float('inf')\n",
    "\n",
    "print(f\"Estimated connection logs size: {logs_size_mb:.2f} MB\")\n",
    "print(f\"Estimated country info size: {country_size_mb:.2f} MB\")\n",
    "print(f\"Size ratio (logs/country): {size_ratio:.2f}\")\n",
    "\n",
    "# Check against broadcast threshold\n",
    "broadcast_threshold_mb = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\") \n",
    "broadcast_threshold_mb = int(broadcast_threshold_mb) / (1024 * 1024) if broadcast_threshold_mb != \"-1\" else 10  # Default 10MB\n",
    "\n",
    "print(f\"\\nCurrent broadcast threshold: {broadcast_threshold_mb} MB\")\n",
    "\n",
    "if country_size_mb < broadcast_threshold_mb:\n",
    "    print(f\"The country_info DataFrame ({country_size_mb:.2f} MB) is below the broadcast threshold and can be automatically broadcast\")\n",
    "else:\n",
    "    print(f\"The country_info DataFrame ({country_size_mb:.2f} MB) is above the broadcast threshold and will require explicit broadcasting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Join (No Broadcasting)\n",
    "\n",
    "First, let's perform a join without any broadcast hints to establish a baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Perform regular join and measure performance\n",
    "\n",
    "# Disable automatic broadcasting for baseline test\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "# Define the benchmark function\n",
    "def benchmark_join(join_function, name):\n",
    "    start_time = time.time()\n",
    "    result = join_function()\n",
    "    # Force execution\n",
    "    count = result.count()\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"{name} completed in {execution_time:.2f} seconds with {count} results\")\n",
    "    return result, execution_time\n",
    "\n",
    "# Regular join function\n",
    "def regular_join():\n",
    "    return connection_logs \\\n",
    "        .join(country_info, connection_logs.country == country_info.country_code) \\\n",
    "        .groupBy(\"country_name\", \"region\") \\\n",
    "        .agg(count(\"*\").alias(\"connection_count\"), \n",
    "             avg(\"duration_seconds\").alias(\"avg_duration\"))\n",
    "\n",
    "# Run regular join benchmark\n",
    "regular_result, regular_time = benchmark_join(regular_join, \"Regular join (no broadcast)\")\n",
    "regular_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit Broadcast Join\n",
    "\n",
    "Now let's use the `broadcast` hint to explicitly broadcast the smaller dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Implement explicit broadcasting and measure performance\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Explicit broadcast join function\n",
    "def explicit_broadcast_join():\n",
    "    return connection_logs \\\n",
    "        .join(broadcast(country_info), connection_logs.country == country_info.country_code) \\\n",
    "        .groupBy(\"country_name\", \"region\") \\\n",
    "        .agg(count(\"*\").alias(\"connection_count\"), \n",
    "             avg(\"duration_seconds\").alias(\"avg_duration\"))\n",
    "\n",
    "# Run explicit broadcast join benchmark\n",
    "explicit_result, explicit_time = benchmark_join(explicit_broadcast_join, \"Explicit broadcast join\")\n",
    "explicit_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Broadcast Join\n",
    "\n",
    "Finally, let's re-enable automatic broadcasting and see if Spark chooses to broadcast without explicit hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Enable automatic broadcasting and measure performance\n",
    "\n",
    "# Re-enable automatic broadcasting\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10 * 1024 * 1024)  # 10 MB\n",
    "\n",
    "# Automatic broadcast join function (same as regular but with auto-broadcast enabled)\n",
    "def auto_broadcast_join():\n",
    "    return connection_logs \\\n",
    "        .join(country_info, connection_logs.country == country_info.country_code) \\\n",
    "        .groupBy(\"country_name\", \"region\") \\\n",
    "        .agg(count(\"*\").alias(\"connection_count\"), \n",
    "             avg(\"duration_seconds\").alias(\"avg_duration\"))\n",
    "\n",
    "# Run automatic broadcast join benchmark\n",
    "auto_result, auto_time = benchmark_join(auto_broadcast_join, \"Automatic broadcast join\")\n",
    "auto_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Query Plans\n",
    "\n",
    "Let's look at the query plans to confirm whether broadcasting is occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Compare execution plans\n",
    "\n",
    "# Examine regular join plan\n",
    "print(\"\\nRegular Join Plan:\")\n",
    "regular_plan = connection_logs \\\n",
    "    .join(country_info, connection_logs.country == country_info.country_code) \\\n",
    "    .groupBy(\"country_name\", \"region\") \\\n",
    "    .agg(count(\"*\").alias(\"connection_count\"))\n",
    "    \n",
    "regular_plan.explain()\n",
    "\n",
    "# Examine explicit broadcast join plan\n",
    "print(\"\\nExplicit Broadcast Join Plan:\")\n",
    "explicit_plan = connection_logs \\\n",
    "    .join(broadcast(country_info), connection_logs.country == country_info.country_code) \\\n",
    "    .groupBy(\"country_name\", \"region\") \\\n",
    "    .agg(count(\"*\").alias(\"connection_count\"))\n",
    "    \n",
    "explicit_plan.explain()\n",
    "\n",
    "# Examine automatic broadcast join plan\n",
    "print(\"\\nAutomatic Broadcast Join Plan:\")\n",
    "auto_plan = connection_logs \\\n",
    "    .join(country_info, connection_logs.country == country_info.country_code) \\\n",
    "    .groupBy(\"country_name\", \"region\") \\\n",
    "    .agg(count(\"*\").alias(\"connection_count\"))\n",
    "    \n",
    "auto_plan.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Visualize and compare performance\n",
    "\n",
    "# Plot performance comparison\n",
    "join_types = ['Regular Join', 'Explicit Broadcast', 'Automatic Broadcast']\n",
    "execution_times = [regular_time, explicit_time, auto_time]\n",
    "\n",
    "# Calculate performance improvements\n",
    "explicit_improvement = ((regular_time - explicit_time) / regular_time) * 100\n",
    "auto_improvement = ((regular_time - auto_time) / regular_time) * 100\n",
    "\n",
    "print(f\"Explicit broadcast join improvement: {explicit_improvement:.2f}%\")\n",
    "print(f\"Automatic broadcast join improvement: {auto_improvement:.2f}%\")\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(join_types, execution_times, color=['red', 'green', 'blue'])\n",
    "\n",
    "# Add labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{height:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Join Performance Comparison')\n",
    "plt.ylim(0, max(execution_times) * 1.2)  # Add some headroom for labels\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: When to Use Broadcasting\n",
    "\n",
    "Broadcast joins are effective when:\n",
    "\n",
    "1. One dataset is significantly smaller than the other\n",
    "2. The small dataset fits in memory on each executor\n",
    "3. The join is selective (not a cross join)\n",
    "\n",
    "The default threshold in Spark for automatic broadcasting is 10MB, but this can be adjusted based on your cluster's available memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Experiment with different broadcast thresholds\n",
    "\n",
    "# Test with larger threshold\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024)  # 50 MB\n",
    "print(f\"Set broadcast threshold to 50 MB\")\n",
    "\n",
    "# Run the join with larger threshold\n",
    "large_threshold_result, large_threshold_time = benchmark_join(auto_broadcast_join, \"Join with 50 MB threshold\")\n",
    "\n",
    "# Reset to default\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10 * 1024 * 1024)  # 10 MB\n",
    "\n",
    "# Check if the plan shows broadcast with the larger threshold\n",
    "print(\"\\nJoin Plan with 50 MB Threshold:\")\n",
    "connection_logs \\\n",
    "    .join(country_info, connection_logs.country == country_info.country_code) \\\n",
    "    .groupBy(\"country_name\", \"region\") \\\n",
    "    .agg(count(\"*\").alias(\"connection_count\")) \\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Broadcast Join Trade-offs\n",
    "\n",
    "While broadcast joins can significantly improve performance, they come with limitations:\n",
    "\n",
    "1. Memory limitations: The broadcast table must fit in memory\n",
    "2. Broadcast overhead: The table must be distributed to all executors\n",
    "3. Not suitable for frequent updates: Broadcasting is inefficient for tables that change frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Discuss memory implications of broadcast joins\n",
    "\n",
    "# Calculate memory needed for broadcasting with different dataset sizes\n",
    "def calculate_broadcast_memory(row_count, row_size_bytes, executor_count):\n",
    "    \"\"\"Calculate memory needed to broadcast a dataset\"\"\"\n",
    "    total_size_bytes = row_count * row_size_bytes\n",
    "    # Each executor needs a copy plus overhead\n",
    "    broadcast_overhead = 1.5  # 50% overhead for broadcast variables\n",
    "    memory_needed_bytes = total_size_bytes * broadcast_overhead * executor_count\n",
    "    return memory_needed_bytes / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Example for our country_info table\n",
    "row_count = country_info.count()\n",
    "avg_row_size = 200  # bytes, estimated\n",
    "executor_counts = [2, 5, 10, 20, 50, 100]\n",
    "\n",
    "print(\"Memory needed to broadcast country_info table:\")\n",
    "for executor_count in executor_counts:\n",
    "    memory_mb = calculate_broadcast_memory(row_count, avg_row_size, executor_count)\n",
    "    print(f\"  With {executor_count} executors: {memory_mb:.2f} MB\")\n",
    "\n",
    "# Example for a larger dimension table\n",
    "large_dim_rows = 1000000  # 1 million rows\n",
    "large_row_size = 500  # bytes\n",
    "\n",
    "print(\"\\nMemory needed to broadcast a large dimension table (1M rows):\")\n",
    "for executor_count in executor_counts:\n",
    "    memory_mb = calculate_broadcast_memory(large_dim_rows, large_row_size, executor_count)\n",
    "    memory_gb = memory_mb / 1024\n",
    "    print(f\"  With {executor_count} executors: {memory_gb:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
