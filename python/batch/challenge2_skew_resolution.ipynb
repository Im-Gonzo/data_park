{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2: Resolving Data Skew with Salting Techniques\n",
    "\n",
    "## Task Description\n",
    "In this challenge, we need to:\n",
    "1. Implement key salting to distribute skewed data more evenly\n",
    "2. Compare performance before and after salting\n",
    "3. Apply the correct salt factor based on skew severity\n",
    "4. Maintain data integrity while resolving skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Skew Resolution\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Skewed Connection Log Data\n",
    "\n",
    "Let's recreate or load our skewed connection logs from Challenge 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate skewed data similar to Challenge 1\n",
    "countries = [\"US\", \"UK\", \"DE\", \"FR\", \"CN\", \"IN\", \"BR\", \"JP\", \"CA\", \"AU\"]\n",
    "    \n",
    "# Create sample data with skew\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "    \n",
    "data = []\n",
    "    \n",
    "# Generate connection records with skew\n",
    "num_records = 100000\n",
    "    \n",
    "skewed_country = \"US\"  # This country will have most of the records\n",
    "skew_percentage = 0.7  # 70% of records will be for this country\n",
    "    \n",
    "for i in range(num_records):\n",
    "    # Determine country with skew\n",
    "    if random.random() < skew_percentage:\n",
    "        country = skewed_country\n",
    "    else:\n",
    "        country = random.choice([c for c in countries if c != skewed_country])\n",
    "    \n",
    "    # Create record\n",
    "    timestamp = datetime.now() - timedelta(days=random.randint(0, 30), \n",
    "                                          hours=random.randint(0, 23),\n",
    "                                          minutes=random.randint(0, 59))\n",
    "    \n",
    "    data.append((\n",
    "        f\"user_{random.randint(1, 1000)}\",  # user_id\n",
    "        timestamp.isoformat(),              # timestamp\n",
    "        country,                            # country\n",
    "        f\"10.{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}\",  # ip_address\n",
    "        random.choice([\"success\", \"failed\"]), # status\n",
    "        random.randint(1, 100)              # duration_seconds\n",
    "    ))\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"user_id\", \"timestamp\", \"country\", \"ip_address\", \"status\", \"duration_seconds\"]\n",
    "connection_logs = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Also create the country info DataFrame\n",
    "country_info = spark.createDataFrame([\n",
    "    (\"US\", \"United States\", \"North America\", \"English\"),\n",
    "    (\"UK\", \"United Kingdom\", \"Europe\", \"English\"),\n",
    "    (\"DE\", \"Germany\", \"Europe\", \"German\"),\n",
    "    (\"FR\", \"France\", \"Europe\", \"French\"),\n",
    "    (\"CN\", \"China\", \"Asia\", \"Chinese\"),\n",
    "    (\"IN\", \"India\", \"Asia\", \"Hindi/English\"),\n",
    "    (\"BR\", \"Brazil\", \"South America\", \"Portuguese\"),\n",
    "    (\"JP\", \"Japan\", \"Asia\", \"Japanese\"),\n",
    "    (\"CA\", \"Canada\", \"North America\", \"English/French\"),\n",
    "    (\"AU\", \"Australia\", \"Oceania\", \"English\")\n",
    "], [\"country_code\", \"country_name\", \"region\", \"language\"])\n",
    "\n",
    "# Cache for better performance\n",
    "connection_logs.cache()\n",
    "country_info.cache()\n",
    "\n",
    "print(f\"Created {connection_logs.count()} sample records with skewed distribution\")\n",
    "connection_logs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Performance (Without Salting)\n",
    "\n",
    "Let's measure the performance without any skew mitigation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a benchmark operation that will be affected by skew\n",
    "def benchmark_join_operation(logs_df, country_df, description):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform join and aggregation\n",
    "    result = logs_df \\\n",
    "        .join(country_df, logs_df.country == country_df.country_code) \\\n",
    "        .groupBy(\"country_name\", \"region\") \\\n",
    "        .agg(count(\"*\").alias(\"connection_count\"))\n",
    "    \n",
    "    # Force execution\n",
    "    result.collect()\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    print(f\"{description}: {execution_time:.2f} seconds\")\n",
    "    \n",
    "    return result, execution_time\n",
    "\n",
    "# Run baseline benchmark\n",
    "baseline_result, baseline_time = benchmark_join_operation(\n",
    "    connection_logs, \n",
    "    country_info,\n",
    "    \"Baseline join time (without salting)\"\n",
    ")\n",
    "\n",
    "baseline_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Key Salting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Implement a key salting strategy for the skewed country\n",
    "\n",
    "# First identify the skewed keys\n",
    "country_distribution = connection_logs \\\n",
    "    .groupBy(\"country\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\"))\n",
    "\n",
    "country_distribution.show()\n",
    "\n",
    "# Get the most skewed country and count\n",
    "skewed_countries = country_distribution.collect()\n",
    "most_skewed_country = skewed_countries[0][\"country\"]\n",
    "most_skewed_count = skewed_countries[0][\"count\"]\n",
    "total_records = connection_logs.count()\n",
    "\n",
    "print(f\"Most skewed country: {most_skewed_country} with {most_skewed_count} records\")\n",
    "print(f\"Percentage: {(most_skewed_count / total_records) * 100:.2f}%\")\n",
    "\n",
    "# Determine appropriate salt factor based on skew\n",
    "skew_ratio = most_skewed_count / (total_records / 10)  # Assuming even distribution across 10 countries\n",
    "salt_factor = max(2, min(10, int(skew_ratio)))  # Limit between 2 and 10\n",
    "\n",
    "print(f\"Using salt factor: {salt_factor} based on skew ratio: {skew_ratio:.2f}\")\n",
    "\n",
    "# TODO: Define a function to add salting to skewed keys\n",
    "def salt_skewed_key(df, skewed_key_col, skewed_key_value, salt_factor):\n",
    "    \"\"\"\n",
    "    Add salting to skewed keys by creating multiple virtual keys.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        skewed_key_col: Column name containing the skewed key\n",
    "        skewed_key_value: Value of the key that is skewed\n",
    "        salt_factor: Number of virtual keys to create\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with salted keys\n",
    "    \"\"\"\n",
    "    # Add a salt value (0 to salt_factor-1) for skewed keys\n",
    "    return df.withColumn(\n",
    "        \"salted_key\",\n",
    "        when(\n",
    "            col(skewed_key_col) == skewed_key_value,\n",
    "            concat(col(skewed_key_col), lit(\"_\"), (rand() * salt_factor).cast(\"int\").cast(\"string\"))\n",
    "        ).otherwise(col(skewed_key_col))\n",
    "    )\n",
    "\n",
    "# Apply salting to the connection logs\n",
    "salted_logs = salt_skewed_key(connection_logs, \"country\", most_skewed_country, salt_factor)\n",
    "\n",
    "# Also salt the country info for the join\n",
    "expanded_country_info = country_info\n",
    "\n",
    "# For the skewed country, create multiple rows with salted keys\n",
    "skewed_country_row = country_info.filter(col(\"country_code\") == most_skewed_country).collect()[0]\n",
    "salt_rows = []\n",
    "\n",
    "for i in range(salt_factor):\n",
    "    salt_rows.append((f\"{most_skewed_country}_{i}\", \n",
    "                      skewed_country_row[\"country_name\"], \n",
    "                      skewed_country_row[\"region\"], \n",
    "                      skewed_country_row[\"language\"]))\n",
    "\n",
    "# Create DataFrame with salt rows\n",
    "salt_df = spark.createDataFrame(salt_rows, country_info.schema)\n",
    "\n",
    "# Remove the original skewed country row and union with salted rows\n",
    "expanded_country_info = country_info.filter(col(\"country_code\") != most_skewed_country) \\\n",
    "    .union(salt_df)\n",
    "\n",
    "# Show the salted data\n",
    "print(\"Sample of salted connection logs:\")\n",
    "salted_logs.select(\"country\", \"salted_key\").show(10)\n",
    "\n",
    "print(\"Expanded country info:\")\n",
    "expanded_country_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Distribution After Salting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Verify that salting has improved the distribution\n",
    "salted_distribution = salted_logs \\\n",
    "    .groupBy(\"salted_key\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\"))\n",
    "\n",
    "salted_distribution.show(20)\n",
    "\n",
    "# Visualize the before and after distribution\n",
    "country_data = country_distribution.collect()\n",
    "salted_data = salted_distribution.collect()\n",
    "\n",
    "# Extract data for plotting\n",
    "original_keys = [row[\"country\"] for row in country_data]\n",
    "original_counts = [row[\"count\"] for row in country_data]\n",
    "\n",
    "salted_keys = [row[\"salted_key\"] for row in salted_data]\n",
    "salted_counts = [row[\"count\"] for row in salted_data]\n",
    "\n",
    "# Create a side-by-side bar chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "ax1.bar(original_keys, original_counts)\n",
    "ax1.set_title('Original Distribution')\n",
    "ax1.set_xlabel('Country')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax2.bar(salted_keys, salted_counts)\n",
    "ax2.set_title('Salted Distribution')\n",
    "ax2.set_xlabel('Salted Key')\n",
    "ax2.tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salted Join Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Measure performance with salting\n",
    "\n",
    "# Define a function for salted join\n",
    "def salted_join_benchmark(salted_logs, expanded_country_info):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Join on the salted key\n",
    "    result = salted_logs \\\n",
    "        .join(expanded_country_info, salted_logs.salted_key == expanded_country_info.country_code) \\\n",
    "        .groupBy(\"country_name\", \"region\") \\\n",
    "        .agg(count(\"*\").alias(\"connection_count\"))\n",
    "    \n",
    "    # Force execution\n",
    "    result.collect()\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    print(f\"Salted join time: {execution_time:.2f} seconds\")\n",
    "    \n",
    "    return result, execution_time\n",
    "\n",
    "# Run the salted join benchmark\n",
    "salted_result, salted_time = salted_join_benchmark(salted_logs, expanded_country_info)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nResults verification (should match):\")\n",
    "print(\"Baseline result:\")\n",
    "baseline_result.show()\n",
    "\n",
    "print(\"Salted result:\")\n",
    "salted_result.show()\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = ((baseline_time - salted_time) / baseline_time) * 100\n",
    "print(f\"\\nPerformance improvement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Salting Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Implement an alternative salting approach\n",
    "# Instead of random distribution, use a deterministic salt based on a different field\n",
    "\n",
    "def deterministic_salt(df, skewed_key_col, skewed_key_value, secondary_field, salt_factor):\n",
    "    \"\"\"\n",
    "    Add salting to skewed keys based on a hash of another column.\n",
    "    This ensures consistent salting when data is reprocessed.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        skewed_key_col: Column name containing the skewed key\n",
    "        skewed_key_value: Value of the key that is skewed\n",
    "        secondary_field: Another column to use for consistent salting\n",
    "        salt_factor: Number of virtual keys to create\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with deterministically salted keys\n",
    "    \"\"\"\n",
    "    # Hash the secondary field to get consistent salting\n",
    "    return df.withColumn(\n",
    "        \"salted_key\",\n",
    "        when(\n",
    "            col(skewed_key_col) == skewed_key_value,\n",
    "            concat(\n",
    "                col(skewed_key_col), \n",
    "                lit(\"_\"), \n",
    "                (abs(hash(col(secondary_field))) % salt_factor).cast(\"string\")\n",
    "            )\n",
    "        ).otherwise(col(skewed_key_col))\n",
    "    )\n",
    "\n",
    "# Apply deterministic salting using user_id\n",
    "det_salted_logs = deterministic_salt(\n",
    "    connection_logs, \n",
    "    \"country\", \n",
    "    most_skewed_country, \n",
    "    \"user_id\", \n",
    "    salt_factor\n",
    ")\n",
    "\n",
    "# Check distribution\n",
    "det_salted_distribution = det_salted_logs \\\n",
    "    .groupBy(\"salted_key\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\"))\n",
    "\n",
    "det_salted_distribution.show(20)\n",
    "\n",
    "# Benchmark deterministic salting\n",
    "det_salted_result, det_salted_time = salted_join_benchmark(det_salted_logs, expanded_country_info)\n",
    "\n",
    "# Calculate improvement\n",
    "det_improvement = ((baseline_time - det_salted_time) / baseline_time) * 100\n",
    "print(f\"\\nDeterministic salting improvement: {det_improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Visualize the performance differences\n",
    "\n",
    "approaches = ['Baseline (No Salting)', 'Random Salting', 'Deterministic Salting']\n",
    "times = [baseline_time, salted_time, det_salted_time]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(approaches, times, color=['red', 'green', 'blue'])\n",
    "\n",
    "# Add labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{height:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Performance Comparison of Skew Handling Approaches')\n",
    "plt.ylim(0, max(times) * 1.2)  # Add some headroom for labels\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
