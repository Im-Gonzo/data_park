apiVersion: v1
kind: ConfigMap
metadata:
  name: jupyter-spark-init
  namespace: data-park
data:
  init-spark.py: |
    # PostgreSQL connection setup for Spark
    def create_spark_session():
        import os
        from pyspark.sql import SparkSession
        
        # Configure Spark with PostgreSQL driver
        spark = SparkSession.builder \
            .appName("Data Park") \
            .config("spark.jars", "/opt/spark-jars/postgresql-42.5.0.jar") \
            .config("spark.driver.extraClassPath", "/opt/spark-jars/postgresql-42.5.0.jar") \
            .master("spark://spark-master:7077") \
            .getOrCreate()
            
        # Set log level
        spark.sparkContext.setLogLevel("INFO")
        
        # Test connection function
        def test_postgres_connection():
            try:
                df = spark.read \
                    .format("jdbc") \
                    .option("url", "jdbc:postgresql://postgres:5432/datamart") \
                    .option("query", "SELECT 1 as test") \
                    .option("user", "spark") \
                    .option("password", "spark") \
                    .option("driver", "org.postgresql.Driver") \
                    .load()
                
                df.show()
                print("✅ PostgreSQL connection successful!")
            except Exception as e:
                print("❌ PostgreSQL connection failed:")
                print(str(e))
        
        # Return the session
        return spark
    
    # Create a spark session
    spark = create_spark_session()
    
    # Define a helper function for PostgreSQL reads
    def read_postgres_table(table_name, schema="raw"):
        """
        Read a table from PostgreSQL
        
        Args:
            table_name (str): Name of the table
            schema (str): Schema name (default: raw)
            
        Returns:
            DataFrame: Spark DataFrame with table data
        """
        return spark.read \
            .format("jdbc") \
            .option("url", "jdbc:postgresql://postgres:5432/datamart") \
            .option("dbtable", f"{schema}.{table_name}") \
            .option("user", "spark") \
            .option("password", "spark") \
            .option("driver", "org.postgresql.Driver") \
            .load()
    
    # Example usage
    print("✨ Spark session initialized with PostgreSQL support")
    print("Example usage: df = read_postgres_table('customers')")
