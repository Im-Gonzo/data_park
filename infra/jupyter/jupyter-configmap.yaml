apiVersion: v1
kind: ConfigMap
metadata:
  name: jupyter-samples
  namespace: data-park
data:
  sample-spark-batch.ipynb: |
    {
     "cells": [
      {
       "cell_type": "markdown",
       "metadata": {},
       "source": [
        "# Sample Spark Batch Processing Notebook\n",
        "\n",
        "This notebook demonstrates how to use Apache Spark for batch processing."
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Sample Batch Processing\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.driver.memory\", \"1g\") \\\n",
        "    .config(\"spark.executor.memory\", \"1g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark session created\")\n",
        "print(f\"Spark version: {spark.version}\")"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "source": [
        "# Sample data\n",
        "data = [(\"John\", 30), (\"Alice\", 25), (\"Bob\", 35)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "df.show()"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "source": [
        "# Perform some transformations\n",
        "df_filtered = df.filter(df.age > 25)\n",
        "df_filtered.show()"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "source": [
        "# Stop the Spark session\n",
        "spark.stop()"
       ]
      }
     ],
     "metadata": {
      "kernelspec": {
       "display_name": "Python 3",
       "language": "python",
       "name": "python3"
      },
      "language_info": {
       "codemirror_mode": {
        "name": "ipython",
        "version": 3
       },
       "file_extension": ".py",
       "mimetype": "text/x-python",
       "name": "python",
       "nbconvert_exporter": "python",
       "pygments_lexer": "ipython3",
       "version": "3.8.10"
      }
     },
     "nbformat": 4,
     "nbformat_minor": 4
    }
  
  sample-kafka-streaming.ipynb: |
    {
     "cells": [
      {
       "cell_type": "markdown",
       "metadata": {},
       "source": [
        "# Sample Kafka Streaming Notebook\n",
        "\n",
        "This notebook demonstrates how to use Spark Structured Streaming with Kafka."
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, from_json\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Kafka Streaming\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\") \\\n",
        "    .config(\"spark.driver.memory\", \"1g\") \\\n",
        "    .config(\"spark.executor.memory\", \"1g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark session created\")"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "source": [
        "# Define schema for the incoming data\n",
        "schema = StructType([\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"value\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "# Read from Kafka\n",
        "df = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
        "    .option(\"subscribe\", \"customer-events\") \\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\n",
        "    .load()\n",
        "\n",
        "# Parse the value column as JSON\n",
        "parsed_df = df.select(\n",
        "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
        ").select(\"data.*\")\n",
        "\n",
        "# Display the streaming data\n",
        "query = parsed_df \\\n",
        "    .writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "# Wait for the query to terminate\n",
        "# Note: In a notebook, you'll need to stop this cell manually\n",
        "# query.awaitTermination()"
       ]
      }
     ],
     "metadata": {
      "kernelspec": {
       "display_name": "Python 3",
       "language": "python",
       "name": "python3"
      },
      "language_info": {
       "codemirror_mode": {
        "name": "ipython",
        "version": 3
       },
       "file_extension": ".py",
       "mimetype": "text/x-python",
       "name": "python",
       "nbconvert_exporter": "python",
       "pygments_lexer": "ipython3",
       "version": "3.8.10"
      }
     },
     "nbformat": 4,
     "nbformat_minor": 4
    }
  
  postgres-connection.ipynb: |
    {
     "cells": [
      {
       "cell_type": "markdown",
       "metadata": {},
       "source": [
        "# PostgreSQL Connection with Spark\n",
        "\n",
        "This notebook demonstrates how to connect Spark to PostgreSQL."
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PostgreSQL Connection\") \\\n",
        "    .master(\"spark://spark-master:7077\") \\\n",
        "    .config(\"spark.driver.memory\", \"1g\") \\\n",
        "    .config(\"spark.executor.memory\", \"1g\") \\\n",
        "    .config(\"spark.jars\", \"/opt/spark-jars/postgresql-42.6.0.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark session created\")"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "source": [
        "# Read data from PostgreSQL\n",
        "df = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:postgresql://postgres:5432/datamart\") \\\n",
        "    .option(\"dbtable\", \"raw.customers\") \\\n",
        "    .option(\"user\", \"spark\") \\\n",
        "    .option(\"password\", \"spark\") \\\n",
        "    .load()\n",
        "\n",
        "df.show()"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "source": [
        "# Write data to PostgreSQL\n",
        "# Create a sample DataFrame\n",
        "data = [(1, \"John Doe\", \"john@example.com\"), \n",
        "        (2, \"Jane Smith\", \"jane@example.com\"), \n",
        "        (3, \"Mike Johnson\", \"mike@example.com\")]\n",
        "columns = [\"id\", \"name\", \"email\"]\n",
        "new_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Write to PostgreSQL\n",
        "new_df.write \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:postgresql://postgres:5432/datamart\") \\\n",
        "    .option(\"dbtable\", \"processed.users\") \\\n",
        "    .option(\"user\", \"spark\") \\\n",
        "    .option(\"password\", \"spark\") \\\n",
        "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save()"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "source": [
        "# Stop the Spark session\n",
        "spark.stop()"
       ]
      }
     ],
     "metadata": {
      "kernelspec": {
       "display_name": "Python 3",
       "language": "python",
       "name": "python3"
      },
      "language_info": {
       "codemirror_mode": {
        "name": "ipython",
        "version": 3
       },
       "file_extension": ".py",
       "mimetype": "text/x-python",
       "name": "python",
       "nbconvert_exporter": "python",
       "pygments_lexer": "ipython3",
       "version": "3.8.10"
      }
     },
     "nbformat": 4,
     "nbformat_minor": 4
    }
