apiVersion: v1
kind: ConfigMap
metadata:
  name: jupyter-configmap
  namespace: data-park
data:
  init.sh: |
    #!/bin/bash
    
    # Wait for notebook to be ready
    sleep 5
    
    # Create directories
    mkdir -p /home/jovyan/work/streaming
    mkdir -p /home/jovyan/work/batch
    mkdir -p /home/jovyan/work/etl
    mkdir -p /home/jovyan/work/elt
    mkdir -p /home/jovyan/work/generators
    
    # Copy custom files
    cp -r /python/* /home/jovyan/work/
    
    echo "Jupyter environment initialized with data_park Python files"
    
  jupyter_spark_config.py: |
    # Configuration script for PySpark in Jupyter
    
    import os
    from pyspark.sql import SparkSession
    
    def create_spark_session(app_name="Data Park Notebook", use_local=True, packages=None):
        """Create a Spark session configured for the Data Park environment.
        
        Args:
            app_name: Name of the Spark application
            use_local: Whether to use local mode (True) or cluster mode (False)
            packages: Additional packages to include
            
        Returns:
            SparkSession: Configured Spark session
        """
        # Base builder
        builder = SparkSession.builder.appName(app_name)
        
        # Mode configuration
        if use_local:
            builder = builder.master("local[*]")
        else:
            builder = builder.master("spark://spark-master:7077")
        
        # Memory configuration
        builder = builder.config("spark.driver.memory", "1g") \
                        .config("spark.executor.memory", "1g") \
                        .config("spark.sql.shuffle.partitions", "8")
        
        # Package configuration
        if packages:
            builder = builder.config("spark.jars.packages", packages)
            
        # JDBC driver for PostgreSQL
        builder = builder.config("spark.jars", "/tmp/postgresql-42.5.0.jar")
        
        # Create and return session
        return builder.getOrCreate()
    
    # Download PostgreSQL JDBC driver if needed
    def ensure_postgres_driver():
        """Ensure PostgreSQL JDBC driver is available"""
        import os
        import urllib.request
        
        driver_path = "/tmp/postgresql-42.5.0.jar"
        
        if not os.path.exists(driver_path):
            print("Downloading PostgreSQL JDBC driver...")
            url = "https://jdbc.postgresql.org/download/postgresql-42.5.0.jar"
            urllib.request.urlretrieve(url, driver_path)
            print(f"Driver downloaded to {driver_path}")
        else:
            print("PostgreSQL JDBC driver already available")
            
        return driver_path
    
    # Pre-download the driver
    ensure_postgres_driver()
    
    # Create convenience function for streaming
    def create_streaming_session(app_name="Streaming App"):
        """Create a Spark session configured for streaming with Kafka"""
        return create_spark_session(
            app_name=app_name, 
            use_local=True,
            packages="org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1"
        )
    
    print("Spark helper functions loaded. Use create_spark_session() or create_streaming_session()")
